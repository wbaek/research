{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorpack as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from tensorflow.python.ops import gradients_impl\n",
    "from tensorflow.python.ops import array_ops, tensor_array_ops, control_flow_ops\n",
    "\n",
    "def hessians_highrank(ys, xs, gradients=None, name=\"hessians\", colocate_gradients_with_ops=False,\n",
    "            gate_gradients=False, aggregation_method=None):\n",
    "  \"\"\"Constructs the Hessian (one or more rank matrix) of sum of `ys` with respect to `x` in `xs`.\n",
    "  `hessians_highrank()` adds ops to the graph to output the Hessian matrix of `ys`\n",
    "  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\n",
    "  where each tensor is the Hessian of `sum(ys)`. This function currently\n",
    "  only supports evaluating the Hessian with respect to (a list of) one-\n",
    "  dimensional tensors.\n",
    "  The Hessian is a matrix of second-order partial derivatives of a scalar\n",
    "  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n",
    "  Args:\n",
    "    ys: A `Tensor` or list of tensors to be differentiated.\n",
    "    xs: A `Tensor` or list of tensors to be used for differentiation.\n",
    "    name: Optional name to use for grouping all the gradient ops together.\n",
    "      defaults to 'hessians'.\n",
    "    colocate_gradients_with_ops: See `gradients()` documentation for details.\n",
    "    gate_gradients: See `gradients()` documentation for details.\n",
    "    aggregation_method: See `gradients()` documentation for details.\n",
    "  Returns:\n",
    "    A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.\n",
    "  Raises:\n",
    "    LookupError: if one of the operations between `xs` and `ys` does not\n",
    "      have a registered gradient function.\n",
    "  \"\"\"\n",
    "  xs = gradients_impl._AsList(xs)\n",
    "  kwargs = {\n",
    "    'colocate_gradients_with_ops': colocate_gradients_with_ops,\n",
    "    'gate_gradients': gate_gradients,\n",
    "    'aggregation_method': aggregation_method\n",
    "  }\n",
    "  # Compute first-order derivatives and iterate for each x in xs.\n",
    "  hessians = []\n",
    "  _gradients = tf.gradients(ys, xs, **kwargs) if gradients is None else gradients\n",
    "  for i, _gradient, x in zip(range(len(xs)), _gradients, xs):\n",
    "    shape = x.shape\n",
    "    _gradient = tf.reshape(_gradient, [-1])\n",
    "    \n",
    "    n = tf.size(x)\n",
    "    loop_vars = [\n",
    "      array_ops.constant(0, tf.int32),\n",
    "      tensor_array_ops.TensorArray(x.dtype, n)\n",
    "    ]\n",
    "    _, hessian = control_flow_ops.while_loop(\n",
    "      lambda j, _: j < n,\n",
    "      lambda j, result: (j + 1, result.write(j, tf.gradients(_gradient[j], x)[0])),\n",
    "      loop_vars\n",
    "    )\n",
    "    hessians.append(hessian.stack())\n",
    "  return hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorpack as tp\n",
    "\n",
    "from tensorpack import dataset\n",
    "from tensorpack.dataflow import imgaug, AugmentImageComponent, BatchData, PrefetchData\n",
    "import tensorpack.tfutils.symbolic_functions as symbf\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "class ModelMNIST10x10_simple(object):\n",
    "    def __init__(self, learning_rate=1.0, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.inputs = [\n",
    "            tf.placeholder(tf.float32, shape=(None, 10, 10, 1)),\n",
    "            tf.placeholder(tf.int32, shape=(None,)),\n",
    "            tf.placeholder(tf.float32, shape=(None, 10))\n",
    "        ]\n",
    "        \n",
    "        self.probability, self.cost, self.accuracy = self._build_graph(self.inputs)\n",
    "        self.op = self._get_optimize_operator(self.cost, learning_rate)\n",
    "        self.dataflow = {\n",
    "            'train':self._get_data('train'),\n",
    "            'valid':self._get_data('test'),\n",
    "        }\n",
    "        \n",
    "    def _build_graph(self, inputs):\n",
    "        image, label, vector = inputs\n",
    "        \n",
    "        with slim.arg_scope([slim.layers.fully_connected], weights_regularizer=slim.l2_regularizer(1e-5)):\n",
    "            l = slim.layers.conv2d(image, 8, [3, 3], padding='SAME', scope='conv0' ) # 10x10\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool0') # 5x5\n",
    "            l = slim.layers.conv2d(l, 8, [3, 3], scope='conv1') # 3x3\n",
    "            l = slim.layers.conv2d(l, 8, [3, 3], scope='conv2') # 1x1\n",
    "            l = slim.layers.flatten(l, scope='flatten')\n",
    "            logits = slim.layers.fully_connected(l, 10, activation_fn=None, scope='fc0')\n",
    "\n",
    "        # Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\n",
    "        #cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "        cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=vector)\n",
    "        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n",
    "\n",
    "        prob = tf.nn.softmax(logits, name='prob')\n",
    "        accuracy = symbf.accuracy(logits, label, topk=1)\n",
    "        return prob, cost, accuracy\n",
    "    \n",
    "    def _get_optimize_operator(self, cost, learning_rate=1.0):\n",
    "        var_list = (tf.trainable_variables() + tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n",
    "        var_list += tf.get_collection(tf.GraphKeys._STREAMING_MODEL_PORTS)\n",
    "\n",
    "        processors = [optimizer._get_processor(v) for v in var_list]\n",
    "        var_refs = [p.target() for p in processors]\n",
    "\n",
    "        # compute_gradients\n",
    "        grads = tf.gradients(\n",
    "                cost, var_refs,\n",
    "                grad_ys=None, aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        hessis = hessians_highrank(\n",
    "                 cost, var_refs, gradients=grads,\n",
    "                 aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        \n",
    "        second_order_grads = []\n",
    "        for g, h in zip(grads, hessis):\n",
    "            shape = g.shape\n",
    "            d = int(reduce(lambda a,b: a*b, shape))\n",
    "\n",
    "            g = tf.reshape(g, [d, 1])\n",
    "            h = tf.reshape(h, [d, d]) + (tf.eye(d) * 1e-1)\n",
    "            h_inv = tf.matrix_inverse(h)\n",
    "            grad = tf.matmul(h_inv, g)\n",
    "            grad = tf.reshape(grad, shape)\n",
    "            second_order_grads.append(grad)\n",
    "        grads_and_vars = list(zip(second_order_grads, var_list))\n",
    "        \n",
    "        self.grads = grads\n",
    "        \n",
    "        self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        lr_schedule = {\n",
    "            'step':     [                   50],\n",
    "            'rate':     [0.1*learning_rate, learning_rate],\n",
    "        }\n",
    "        lr_schedule['step'] = ops.convert_n_to_tensor(lr_schedule['step'], tf.int64)\n",
    "        learning_rate = tf.train.piecewise_constant(self.global_step, lr_schedule['step'], lr_schedule['rate'])\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "        #opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def _get_data(self, train_or_test):\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        isTrain = train_or_test == 'train'\n",
    "        ds = dataset.Mnist(train_or_test)\n",
    "        if isTrain:\n",
    "            augmentors = [\n",
    "                #imgaug.RandomApplyAug(imgaug.RandomResize((0.8, 1.2), (0.8, 1.2)), 0.3),\n",
    "                #imgaug.RandomApplyAug(imgaug.RotationAndCropValid(15), 0.5),\n",
    "                #imgaug.RandomApplyAug(imgaug.SaltPepperNoise(white_prob=0.01, black_prob=0.01), 0.25),\n",
    "                imgaug.Resize((10, 10)),\n",
    "                imgaug.CenterPaste((12, 12)),\n",
    "                imgaug.RandomCrop((10, 10)),\n",
    "                imgaug.MapImage(lambda x: x.reshape(10, 10, 1))\n",
    "            ]\n",
    "        else:\n",
    "            augmentors = [\n",
    "                imgaug.Resize((10, 10)),\n",
    "                imgaug.MapImage(lambda x: x.reshape(10, 10, 1))\n",
    "            ]\n",
    "        ds = AugmentImageComponent(ds, augmentors)\n",
    "        ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n",
    "        if isTrain:\n",
    "            ds = PrefetchData(ds, 3, 2)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1016 16:47:36 @fs.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n"
     ]
    }
   ],
   "source": [
    "model = ModelMNIST10x10_simple(learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session initialized\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('session initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0000 step:0001 cost:0.695 accuracy:0.094\n",
      "[train] epoch:0000 step:0002 cost:0.624 accuracy:0.117\n",
      "[train] epoch:0000 step:0003 cost:0.456 accuracy:0.125\n",
      "[train] epoch:0000 step:0004 cost:0.341 accuracy:0.117\n",
      "[train] epoch:0000 step:0005 cost:0.423 accuracy:0.055\n",
      "[train] epoch:0000 step:0006 cost:0.333 accuracy:0.148\n",
      "[train] epoch:0000 step:0007 cost:0.335 accuracy:0.125\n",
      "[train] epoch:0000 step:0008 cost:0.340 accuracy:0.109\n",
      "[train] epoch:0000 step:0009 cost:0.338 accuracy:0.109\n",
      "[train] epoch:0000 step:0010 cost:0.329 accuracy:0.156\n",
      "[train] epoch:0000 step:0011 cost:0.323 accuracy:0.148\n",
      "[train] epoch:0000 step:0012 cost:0.324 accuracy:0.109\n",
      "[train] epoch:0000 step:0013 cost:0.319 accuracy:0.141\n",
      "[train] epoch:0000 step:0014 cost:0.318 accuracy:0.203\n",
      "[train] epoch:0000 step:0015 cost:0.315 accuracy:0.195\n",
      "[train] epoch:0000 step:0016 cost:0.309 accuracy:0.344\n",
      "[train] epoch:0000 step:0017 cost:0.311 accuracy:0.281\n",
      "[train] epoch:0000 step:0018 cost:0.310 accuracy:0.312\n",
      "[train] epoch:0000 step:0019 cost:0.308 accuracy:0.344\n",
      "[train] epoch:0000 step:0020 cost:0.305 accuracy:0.352\n",
      "[train] epoch:0000 step:0021 cost:0.291 accuracy:0.438\n",
      "[train] epoch:0000 step:0022 cost:0.288 accuracy:0.469\n",
      "[train] epoch:0000 step:0023 cost:0.280 accuracy:0.359\n",
      "[train] epoch:0000 step:0024 cost:0.273 accuracy:0.453\n",
      "[train] epoch:0000 step:0025 cost:0.269 accuracy:0.469\n",
      "[train] epoch:0000 step:0026 cost:0.265 accuracy:0.383\n",
      "[train] epoch:0000 step:0027 cost:0.246 accuracy:0.500\n",
      "[train] epoch:0000 step:0028 cost:0.243 accuracy:0.469\n",
      "[train] epoch:0000 step:0029 cost:0.235 accuracy:0.516\n",
      "[train] epoch:0000 step:0030 cost:0.212 accuracy:0.609\n",
      "[train] epoch:0000 step:0031 cost:0.247 accuracy:0.492\n",
      "[train] epoch:0000 step:0032 cost:0.208 accuracy:0.609\n",
      "[train] epoch:0000 step:0033 cost:0.235 accuracy:0.547\n",
      "[train] epoch:0000 step:0034 cost:0.212 accuracy:0.539\n",
      "[train] epoch:0000 step:0035 cost:0.218 accuracy:0.570\n",
      "[train] epoch:0000 step:0036 cost:0.186 accuracy:0.648\n",
      "[train] epoch:0000 step:0037 cost:0.192 accuracy:0.641\n",
      "[train] epoch:0000 step:0038 cost:0.220 accuracy:0.539\n",
      "[train] epoch:0000 step:0039 cost:0.191 accuracy:0.633\n",
      "[train] epoch:0000 step:0040 cost:0.183 accuracy:0.648\n",
      "[train] epoch:0000 step:0041 cost:0.213 accuracy:0.562\n",
      "[train] epoch:0000 step:0042 cost:0.169 accuracy:0.695\n",
      "[train] epoch:0000 step:0043 cost:0.184 accuracy:0.609\n",
      "[train] epoch:0000 step:0044 cost:0.185 accuracy:0.664\n",
      "[train] epoch:0000 step:0045 cost:0.197 accuracy:0.648\n",
      "[train] epoch:0000 step:0046 cost:0.162 accuracy:0.703\n",
      "[train] epoch:0000 step:0047 cost:0.178 accuracy:0.609\n",
      "[train] epoch:0000 step:0048 cost:0.167 accuracy:0.664\n",
      "[train] epoch:0000 step:0049 cost:0.166 accuracy:0.695\n",
      "[train] epoch:0000 step:0050 cost:0.157 accuracy:0.711\n",
      "[train] epoch:0000 step:0051 cost:0.161 accuracy:0.719\n",
      "[train] epoch:0000 step:0052 cost:0.164 accuracy:0.688\n",
      "[train] epoch:0000 step:0053 cost:0.170 accuracy:0.703\n",
      "[train] epoch:0000 step:0054 cost:0.150 accuracy:0.703\n",
      "[train] epoch:0000 step:0055 cost:0.136 accuracy:0.711\n",
      "[train] epoch:0000 step:0056 cost:0.158 accuracy:0.703\n",
      "[train] epoch:0000 step:0057 cost:0.156 accuracy:0.719\n",
      "[train] epoch:0000 step:0058 cost:0.140 accuracy:0.758\n",
      "[train] epoch:0000 step:0059 cost:0.155 accuracy:0.703\n",
      "[train] epoch:0000 step:0060 cost:0.133 accuracy:0.766\n",
      "[train] epoch:0000 step:0061 cost:0.142 accuracy:0.734\n",
      "[train] epoch:0000 step:0062 cost:0.122 accuracy:0.820\n",
      "[train] epoch:0000 step:0063 cost:0.145 accuracy:0.695\n",
      "[train] epoch:0000 step:0064 cost:0.121 accuracy:0.797\n",
      "[train] epoch:0000 step:0065 cost:0.134 accuracy:0.742\n",
      "[train] epoch:0000 step:0066 cost:0.119 accuracy:0.844\n",
      "[train] epoch:0000 step:0067 cost:0.129 accuracy:0.773\n",
      "[train] epoch:0000 step:0068 cost:0.133 accuracy:0.734\n",
      "[train] epoch:0000 step:0069 cost:0.118 accuracy:0.820\n",
      "[train] epoch:0000 step:0070 cost:0.130 accuracy:0.758\n",
      "[train] epoch:0000 step:0071 cost:0.098 accuracy:0.852\n",
      "[train] epoch:0000 step:0072 cost:0.117 accuracy:0.766\n",
      "[train] epoch:0000 step:0073 cost:0.130 accuracy:0.773\n",
      "[train] epoch:0000 step:0074 cost:0.140 accuracy:0.766\n",
      "[train] epoch:0000 step:0075 cost:0.127 accuracy:0.758\n",
      "[train] epoch:0000 step:0076 cost:0.130 accuracy:0.766\n",
      "[train] epoch:0000 step:0077 cost:0.093 accuracy:0.844\n",
      "[train] epoch:0000 step:0078 cost:0.100 accuracy:0.844\n",
      "[train] epoch:0000 step:0079 cost:0.107 accuracy:0.852\n",
      "[train] epoch:0000 step:0080 cost:0.099 accuracy:0.828\n",
      "[train] epoch:0000 step:0081 cost:0.173 accuracy:0.719\n",
      "[train] epoch:0000 step:0082 cost:0.126 accuracy:0.750\n",
      "[train] epoch:0000 step:0083 cost:0.096 accuracy:0.867\n",
      "[train] epoch:0000 step:0084 cost:0.095 accuracy:0.867\n",
      "[train] epoch:0000 step:0085 cost:0.097 accuracy:0.836\n",
      "[train] epoch:0000 step:0086 cost:0.090 accuracy:0.844\n",
      "[train] epoch:0000 step:0087 cost:0.106 accuracy:0.805\n",
      "[train] epoch:0000 step:0088 cost:0.117 accuracy:0.766\n",
      "[train] epoch:0000 step:0089 cost:0.109 accuracy:0.828\n",
      "[train] epoch:0000 step:0090 cost:0.112 accuracy:0.805\n",
      "[train] epoch:0000 step:0091 cost:0.104 accuracy:0.844\n",
      "[train] epoch:0000 step:0092 cost:0.102 accuracy:0.836\n",
      "[train] epoch:0000 step:0093 cost:0.080 accuracy:0.914\n",
      "[train] epoch:0000 step:0094 cost:0.092 accuracy:0.859\n",
      "[train] epoch:0000 step:0095 cost:0.076 accuracy:0.891\n",
      "[train] epoch:0000 step:0096 cost:0.097 accuracy:0.859\n",
      "[train] epoch:0000 step:0097 cost:0.074 accuracy:0.867\n",
      "[train] epoch:0000 step:0098 cost:0.078 accuracy:0.875\n",
      "[train] epoch:0000 step:0099 cost:0.094 accuracy:0.875\n",
      "[train] epoch:0000 step:0100 cost:0.091 accuracy:0.828\n",
      "[train] epoch:0000 step:0101 cost:0.079 accuracy:0.875\n",
      "[train] epoch:0000 step:0102 cost:0.096 accuracy:0.852\n",
      "[train] epoch:0000 step:0103 cost:0.074 accuracy:0.859\n",
      "[train] epoch:0000 step:0104 cost:0.111 accuracy:0.844\n",
      "[train] epoch:0000 step:0105 cost:0.110 accuracy:0.797\n",
      "[train] epoch:0000 step:0106 cost:0.097 accuracy:0.836\n",
      "[train] epoch:0000 step:0107 cost:0.105 accuracy:0.820\n",
      "[train] epoch:0000 step:0108 cost:0.089 accuracy:0.859\n",
      "[train] epoch:0000 step:0109 cost:0.105 accuracy:0.859\n",
      "[train] epoch:0000 step:0110 cost:0.098 accuracy:0.844\n",
      "[train] epoch:0000 step:0111 cost:0.085 accuracy:0.859\n",
      "[train] epoch:0000 step:0112 cost:0.092 accuracy:0.867\n",
      "[train] epoch:0000 step:0113 cost:0.101 accuracy:0.867\n",
      "[train] epoch:0000 step:0114 cost:0.102 accuracy:0.812\n",
      "[train] epoch:0000 step:0115 cost:0.124 accuracy:0.781\n",
      "[train] epoch:0000 step:0116 cost:0.090 accuracy:0.828\n",
      "[train] epoch:0000 step:0117 cost:0.096 accuracy:0.852\n",
      "[train] epoch:0000 step:0118 cost:0.121 accuracy:0.828\n",
      "[train] epoch:0000 step:0119 cost:0.102 accuracy:0.859\n",
      "[train] epoch:0000 step:0120 cost:0.082 accuracy:0.891\n",
      "[train] epoch:0000 step:0121 cost:0.085 accuracy:0.875\n",
      "[train] epoch:0000 step:0122 cost:0.079 accuracy:0.906\n",
      "[train] epoch:0000 step:0123 cost:0.111 accuracy:0.820\n",
      "[train] epoch:0000 step:0124 cost:0.084 accuracy:0.883\n",
      "[train] epoch:0000 step:0125 cost:0.094 accuracy:0.812\n",
      "[train] epoch:0000 step:0126 cost:0.081 accuracy:0.867\n",
      "[train] epoch:0000 step:0127 cost:0.077 accuracy:0.844\n",
      "[train] epoch:0000 step:0128 cost:0.090 accuracy:0.836\n",
      "[train] epoch:0000 step:0129 cost:0.079 accuracy:0.875\n",
      "[train] epoch:0000 step:0130 cost:0.090 accuracy:0.875\n",
      "[train] epoch:0000 step:0131 cost:0.090 accuracy:0.852\n",
      "[train] epoch:0000 step:0132 cost:0.086 accuracy:0.836\n",
      "[train] epoch:0000 step:0133 cost:0.081 accuracy:0.859\n",
      "[train] epoch:0000 step:0134 cost:0.079 accuracy:0.875\n",
      "[train] epoch:0000 step:0135 cost:0.094 accuracy:0.836\n",
      "[train] epoch:0000 step:0136 cost:0.076 accuracy:0.883\n",
      "[train] epoch:0000 step:0137 cost:0.081 accuracy:0.883\n",
      "[train] epoch:0000 step:0138 cost:0.073 accuracy:0.859\n",
      "[train] epoch:0000 step:0139 cost:0.095 accuracy:0.852\n",
      "[train] epoch:0000 step:0140 cost:0.088 accuracy:0.859\n",
      "[train] epoch:0000 step:0141 cost:0.066 accuracy:0.891\n",
      "[train] epoch:0000 step:0142 cost:0.084 accuracy:0.852\n",
      "[train] epoch:0000 step:0143 cost:0.075 accuracy:0.859\n",
      "[train] epoch:0000 step:0144 cost:0.102 accuracy:0.852\n",
      "[train] epoch:0000 step:0145 cost:0.108 accuracy:0.812\n",
      "[train] epoch:0000 step:0146 cost:0.089 accuracy:0.836\n",
      "[train] epoch:0000 step:0147 cost:0.076 accuracy:0.914\n",
      "[train] epoch:0000 step:0148 cost:0.072 accuracy:0.906\n",
      "[train] epoch:0000 step:0149 cost:0.058 accuracy:0.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0000 step:0150 cost:0.066 accuracy:0.945\n",
      "[train] epoch:0000 step:0151 cost:0.067 accuracy:0.906\n",
      "[train] epoch:0000 step:0152 cost:0.071 accuracy:0.891\n",
      "[train] epoch:0000 step:0153 cost:0.078 accuracy:0.875\n",
      "[train] epoch:0000 step:0154 cost:0.084 accuracy:0.867\n",
      "[train] epoch:0000 step:0155 cost:0.093 accuracy:0.859\n",
      "[train] epoch:0000 step:0156 cost:0.062 accuracy:0.914\n",
      "[train] epoch:0000 step:0157 cost:0.068 accuracy:0.883\n",
      "[train] epoch:0000 step:0158 cost:0.082 accuracy:0.844\n",
      "[train] epoch:0000 step:0159 cost:0.066 accuracy:0.922\n",
      "[train] epoch:0000 step:0160 cost:0.066 accuracy:0.930\n",
      "[train] epoch:0000 step:0161 cost:0.057 accuracy:0.922\n",
      "[train] epoch:0000 step:0162 cost:0.083 accuracy:0.867\n",
      "[train] epoch:0000 step:0163 cost:0.074 accuracy:0.875\n",
      "[train] epoch:0000 step:0164 cost:0.079 accuracy:0.906\n",
      "[train] epoch:0000 step:0165 cost:0.065 accuracy:0.891\n",
      "[train] epoch:0000 step:0166 cost:0.057 accuracy:0.930\n",
      "[train] epoch:0000 step:0167 cost:0.082 accuracy:0.844\n",
      "[train] epoch:0000 step:0168 cost:0.067 accuracy:0.875\n",
      "[train] epoch:0000 step:0169 cost:0.071 accuracy:0.844\n",
      "[train] epoch:0000 step:0170 cost:0.091 accuracy:0.891\n",
      "[train] epoch:0000 step:0171 cost:0.095 accuracy:0.867\n",
      "[train] epoch:0000 step:0172 cost:0.074 accuracy:0.875\n",
      "[train] epoch:0000 step:0173 cost:0.075 accuracy:0.875\n",
      "[train] epoch:0000 step:0174 cost:0.071 accuracy:0.875\n",
      "[train] epoch:0000 step:0175 cost:0.076 accuracy:0.891\n",
      "[train] epoch:0000 step:0176 cost:0.090 accuracy:0.867\n",
      "[train] epoch:0000 step:0177 cost:0.092 accuracy:0.852\n",
      "[train] epoch:0000 step:0178 cost:0.064 accuracy:0.914\n",
      "[train] epoch:0000 step:0179 cost:0.075 accuracy:0.859\n",
      "[train] epoch:0000 step:0180 cost:0.099 accuracy:0.836\n",
      "[train] epoch:0000 step:0181 cost:0.091 accuracy:0.836\n",
      "[train] epoch:0000 step:0182 cost:0.059 accuracy:0.930\n",
      "[train] epoch:0000 step:0183 cost:0.082 accuracy:0.906\n",
      "[train] epoch:0000 step:0184 cost:0.061 accuracy:0.898\n",
      "[train] epoch:0000 step:0185 cost:0.062 accuracy:0.898\n",
      "[train] epoch:0000 step:0186 cost:0.084 accuracy:0.891\n",
      "[train] epoch:0000 step:0187 cost:0.074 accuracy:0.875\n",
      "[train] epoch:0000 step:0188 cost:0.065 accuracy:0.922\n",
      "[train] epoch:0000 step:0189 cost:0.055 accuracy:0.938\n",
      "[train] epoch:0000 step:0190 cost:0.061 accuracy:0.898\n",
      "[train] epoch:0000 step:0191 cost:0.076 accuracy:0.891\n",
      "[train] epoch:0000 step:0192 cost:0.089 accuracy:0.859\n",
      "[train] epoch:0000 step:0193 cost:0.094 accuracy:0.844\n",
      "[train] epoch:0000 step:0194 cost:0.064 accuracy:0.922\n",
      "[train] epoch:0000 step:0195 cost:0.082 accuracy:0.836\n",
      "[train] epoch:0000 step:0196 cost:0.070 accuracy:0.891\n",
      "[train] epoch:0000 step:0197 cost:0.083 accuracy:0.844\n",
      "[train] epoch:0000 step:0198 cost:0.080 accuracy:0.875\n",
      "[train] epoch:0000 step:0199 cost:0.072 accuracy:0.875\n",
      "[train] epoch:0000 step:0200 cost:0.067 accuracy:0.930\n",
      "[train] epoch:0000 step:0201 cost:0.053 accuracy:0.898\n",
      "[train] epoch:0000 step:0202 cost:0.066 accuracy:0.914\n",
      "[train] epoch:0000 step:0203 cost:0.060 accuracy:0.898\n",
      "[train] epoch:0000 step:0204 cost:0.057 accuracy:0.914\n",
      "[train] epoch:0000 step:0205 cost:0.070 accuracy:0.914\n",
      "[train] epoch:0000 step:0206 cost:0.056 accuracy:0.945\n",
      "[train] epoch:0000 step:0207 cost:0.065 accuracy:0.945\n",
      "[train] epoch:0000 step:0208 cost:0.056 accuracy:0.914\n",
      "[train] epoch:0000 step:0209 cost:0.070 accuracy:0.883\n",
      "[train] epoch:0000 step:0210 cost:0.052 accuracy:0.953\n",
      "[train] epoch:0000 step:0211 cost:0.054 accuracy:0.906\n",
      "[train] epoch:0000 step:0212 cost:0.071 accuracy:0.891\n",
      "[train] epoch:0000 step:0213 cost:0.068 accuracy:0.906\n",
      "[train] epoch:0000 step:0214 cost:0.059 accuracy:0.938\n",
      "[train] epoch:0000 step:0215 cost:0.064 accuracy:0.898\n",
      "[train] epoch:0000 step:0216 cost:0.053 accuracy:0.938\n",
      "[train] epoch:0000 step:0217 cost:0.065 accuracy:0.898\n",
      "[train] epoch:0000 step:0218 cost:0.062 accuracy:0.898\n",
      "[train] epoch:0000 step:0219 cost:0.078 accuracy:0.883\n",
      "[train] epoch:0000 step:0220 cost:0.076 accuracy:0.875\n",
      "[train] epoch:0000 step:0221 cost:0.039 accuracy:0.977\n",
      "[train] epoch:0000 step:0222 cost:0.055 accuracy:0.906\n",
      "[train] epoch:0000 step:0223 cost:0.054 accuracy:0.938\n",
      "[train] epoch:0000 step:0224 cost:0.050 accuracy:0.922\n",
      "[train] epoch:0000 step:0225 cost:0.074 accuracy:0.914\n",
      "[train] epoch:0000 step:0226 cost:0.082 accuracy:0.852\n",
      "[train] epoch:0000 step:0227 cost:0.047 accuracy:0.953\n",
      "[train] epoch:0000 step:0228 cost:0.069 accuracy:0.906\n",
      "[train] epoch:0000 step:0229 cost:0.062 accuracy:0.906\n",
      "[train] epoch:0000 step:0230 cost:0.064 accuracy:0.906\n",
      "[train] epoch:0000 step:0231 cost:0.077 accuracy:0.898\n",
      "[train] epoch:0000 step:0232 cost:0.085 accuracy:0.859\n",
      "[train] epoch:0000 step:0233 cost:0.064 accuracy:0.938\n",
      "[train] epoch:0000 step:0234 cost:0.064 accuracy:0.891\n",
      "[train] epoch:0000 step:0235 cost:0.051 accuracy:0.930\n",
      "[train] epoch:0000 step:0236 cost:0.055 accuracy:0.922\n",
      "[train] epoch:0000 step:0237 cost:0.057 accuracy:0.891\n",
      "[train] epoch:0000 step:0238 cost:0.086 accuracy:0.836\n",
      "[train] epoch:0000 step:0239 cost:0.049 accuracy:0.938\n",
      "[train] epoch:0000 step:0240 cost:0.043 accuracy:0.953\n",
      "[train] epoch:0000 step:0241 cost:0.049 accuracy:0.930\n",
      "[train] epoch:0000 step:0242 cost:0.074 accuracy:0.883\n",
      "[train] epoch:0000 step:0243 cost:0.074 accuracy:0.859\n",
      "[train] epoch:0000 step:0244 cost:0.060 accuracy:0.875\n",
      "[train] epoch:0000 step:0245 cost:0.066 accuracy:0.922\n",
      "[train] epoch:0000 step:0246 cost:0.040 accuracy:0.945\n",
      "[train] epoch:0000 step:0247 cost:0.069 accuracy:0.906\n",
      "[train] epoch:0000 step:0248 cost:0.067 accuracy:0.891\n",
      "[train] epoch:0000 step:0249 cost:0.074 accuracy:0.922\n",
      "[train] epoch:0000 step:0250 cost:0.061 accuracy:0.898\n",
      "[train] epoch:0000 step:0251 cost:0.068 accuracy:0.883\n",
      "[train] epoch:0000 step:0252 cost:0.047 accuracy:0.945\n",
      "[train] epoch:0000 step:0253 cost:0.065 accuracy:0.914\n",
      "[train] epoch:0000 step:0254 cost:0.056 accuracy:0.914\n",
      "[train] epoch:0000 step:0255 cost:0.060 accuracy:0.906\n",
      "[train] epoch:0000 step:0256 cost:0.061 accuracy:0.906\n",
      "[train] epoch:0000 step:0257 cost:0.058 accuracy:0.891\n",
      "[train] epoch:0000 step:0258 cost:0.075 accuracy:0.891\n",
      "[train] epoch:0000 step:0259 cost:0.078 accuracy:0.859\n",
      "[train] epoch:0000 step:0260 cost:0.053 accuracy:0.922\n",
      "[train] epoch:0000 step:0261 cost:0.062 accuracy:0.922\n",
      "[train] epoch:0000 step:0262 cost:0.075 accuracy:0.883\n",
      "[train] epoch:0000 step:0263 cost:0.064 accuracy:0.930\n",
      "[train] epoch:0000 step:0264 cost:0.067 accuracy:0.906\n",
      "[train] epoch:0000 step:0265 cost:0.059 accuracy:0.922\n",
      "[train] epoch:0000 step:0266 cost:0.080 accuracy:0.844\n",
      "[train] epoch:0000 step:0267 cost:0.065 accuracy:0.914\n",
      "[train] epoch:0000 step:0268 cost:0.057 accuracy:0.914\n",
      "[train] epoch:0000 step:0269 cost:0.065 accuracy:0.914\n",
      "[train] epoch:0000 step:0270 cost:0.058 accuracy:0.898\n",
      "[train] epoch:0000 step:0271 cost:0.071 accuracy:0.883\n",
      "[train] epoch:0000 step:0272 cost:0.043 accuracy:0.930\n",
      "[train] epoch:0000 step:0273 cost:0.046 accuracy:0.930\n",
      "[train] epoch:0000 step:0274 cost:0.062 accuracy:0.906\n",
      "[train] epoch:0000 step:0275 cost:0.041 accuracy:0.953\n",
      "[train] epoch:0000 step:0276 cost:0.061 accuracy:0.875\n",
      "[train] epoch:0000 step:0277 cost:0.053 accuracy:0.914\n",
      "[train] epoch:0000 step:0278 cost:0.067 accuracy:0.922\n",
      "[train] epoch:0000 step:0279 cost:0.063 accuracy:0.922\n",
      "[train] epoch:0000 step:0280 cost:0.079 accuracy:0.875\n",
      "[train] epoch:0000 step:0281 cost:0.049 accuracy:0.914\n",
      "[train] epoch:0000 step:0282 cost:0.056 accuracy:0.914\n",
      "[train] epoch:0000 step:0283 cost:0.047 accuracy:0.938\n",
      "[train] epoch:0000 step:0284 cost:0.051 accuracy:0.914\n",
      "[train] epoch:0000 step:0285 cost:0.046 accuracy:0.938\n",
      "[train] epoch:0000 step:0286 cost:0.053 accuracy:0.930\n",
      "[train] epoch:0000 step:0287 cost:0.059 accuracy:0.891\n",
      "[train] epoch:0000 step:0288 cost:0.057 accuracy:0.914\n",
      "[train] epoch:0000 step:0289 cost:0.066 accuracy:0.898\n",
      "[train] epoch:0000 step:0290 cost:0.064 accuracy:0.922\n",
      "[train] epoch:0000 step:0291 cost:0.055 accuracy:0.922\n",
      "[train] epoch:0000 step:0292 cost:0.051 accuracy:0.922\n",
      "[train] epoch:0000 step:0293 cost:0.052 accuracy:0.906\n",
      "[train] epoch:0000 step:0294 cost:0.068 accuracy:0.883\n",
      "[train] epoch:0000 step:0295 cost:0.063 accuracy:0.906\n",
      "[train] epoch:0000 step:0296 cost:0.045 accuracy:0.938\n",
      "[train] epoch:0000 step:0297 cost:0.059 accuracy:0.906\n",
      "[train] epoch:0000 step:0298 cost:0.067 accuracy:0.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0000 step:0299 cost:0.083 accuracy:0.891\n",
      "[train] epoch:0000 step:0300 cost:0.066 accuracy:0.891\n",
      "[train] epoch:0000 step:0301 cost:0.062 accuracy:0.898\n",
      "[train] epoch:0000 step:0302 cost:0.039 accuracy:0.953\n",
      "[train] epoch:0000 step:0303 cost:0.066 accuracy:0.883\n",
      "[train] epoch:0000 step:0304 cost:0.065 accuracy:0.906\n",
      "[train] epoch:0000 step:0305 cost:0.074 accuracy:0.875\n",
      "[train] epoch:0000 step:0306 cost:0.043 accuracy:0.961\n",
      "[train] epoch:0000 step:0307 cost:0.062 accuracy:0.906\n",
      "[train] epoch:0000 step:0308 cost:0.061 accuracy:0.906\n",
      "[train] epoch:0000 step:0309 cost:0.054 accuracy:0.922\n",
      "[train] epoch:0000 step:0310 cost:0.039 accuracy:0.969\n",
      "[train] epoch:0000 step:0311 cost:0.035 accuracy:0.953\n",
      "[train] epoch:0000 step:0312 cost:0.089 accuracy:0.859\n",
      "[train] epoch:0000 step:0313 cost:0.071 accuracy:0.906\n",
      "[train] epoch:0000 step:0314 cost:0.097 accuracy:0.844\n",
      "[train] epoch:0000 step:0315 cost:0.045 accuracy:0.945\n",
      "[train] epoch:0000 step:0316 cost:0.057 accuracy:0.914\n",
      "[train] epoch:0000 step:0317 cost:0.073 accuracy:0.891\n",
      "[train] epoch:0000 step:0318 cost:0.061 accuracy:0.883\n",
      "[train] epoch:0000 step:0319 cost:0.061 accuracy:0.906\n",
      "[train] epoch:0000 step:0320 cost:0.068 accuracy:0.914\n",
      "[train] epoch:0000 step:0321 cost:0.059 accuracy:0.891\n",
      "[train] epoch:0000 step:0322 cost:0.061 accuracy:0.914\n",
      "[train] epoch:0000 step:0323 cost:0.050 accuracy:0.914\n",
      "[train] epoch:0000 step:0324 cost:0.048 accuracy:0.930\n",
      "[train] epoch:0000 step:0325 cost:0.064 accuracy:0.875\n",
      "[train] epoch:0000 step:0326 cost:0.056 accuracy:0.945\n",
      "[train] epoch:0000 step:0327 cost:0.059 accuracy:0.930\n",
      "[train] epoch:0000 step:0328 cost:0.057 accuracy:0.906\n",
      "[train] epoch:0000 step:0329 cost:0.074 accuracy:0.883\n",
      "[train] epoch:0000 step:0330 cost:0.062 accuracy:0.898\n",
      "[train] epoch:0000 step:0331 cost:0.051 accuracy:0.930\n",
      "[train] epoch:0000 step:0332 cost:0.070 accuracy:0.883\n",
      "[train] epoch:0000 step:0333 cost:0.067 accuracy:0.922\n",
      "[train] epoch:0000 step:0334 cost:0.043 accuracy:0.953\n",
      "[train] epoch:0000 step:0335 cost:0.060 accuracy:0.898\n",
      "[train] epoch:0000 step:0336 cost:0.051 accuracy:0.914\n",
      "[train] epoch:0000 step:0337 cost:0.050 accuracy:0.891\n",
      "[train] epoch:0000 step:0338 cost:0.045 accuracy:0.938\n",
      "[train] epoch:0000 step:0339 cost:0.060 accuracy:0.914\n",
      "[train] epoch:0000 step:0340 cost:0.063 accuracy:0.891\n",
      "[train] epoch:0000 step:0341 cost:0.066 accuracy:0.914\n",
      "[train] epoch:0000 step:0342 cost:0.051 accuracy:0.938\n",
      "[train] epoch:0000 step:0343 cost:0.067 accuracy:0.906\n",
      "[train] epoch:0000 step:0344 cost:0.058 accuracy:0.898\n",
      "[train] epoch:0000 step:0345 cost:0.059 accuracy:0.914\n",
      "[train] epoch:0000 step:0346 cost:0.057 accuracy:0.945\n",
      "[train] epoch:0000 step:0347 cost:0.072 accuracy:0.898\n",
      "[train] epoch:0000 step:0348 cost:0.058 accuracy:0.914\n",
      "[train] epoch:0000 step:0349 cost:0.057 accuracy:0.938\n",
      "[train] epoch:0000 step:0350 cost:0.065 accuracy:0.906\n",
      "[train] epoch:0000 step:0351 cost:0.077 accuracy:0.883\n",
      "[train] epoch:0000 step:0352 cost:0.052 accuracy:0.914\n",
      "[train] epoch:0000 step:0353 cost:0.047 accuracy:0.930\n",
      "[train] epoch:0000 step:0354 cost:0.046 accuracy:0.945\n",
      "[train] epoch:0000 step:0355 cost:0.042 accuracy:0.922\n",
      "[train] epoch:0000 step:0356 cost:0.064 accuracy:0.922\n",
      "[train] epoch:0000 step:0357 cost:0.065 accuracy:0.891\n",
      "[train] epoch:0000 step:0358 cost:0.058 accuracy:0.914\n",
      "[train] epoch:0000 step:0359 cost:0.047 accuracy:0.914\n",
      "[train] epoch:0000 step:0360 cost:0.068 accuracy:0.906\n",
      "[train] epoch:0000 step:0361 cost:0.060 accuracy:0.906\n",
      "[train] epoch:0000 step:0362 cost:0.056 accuracy:0.898\n",
      "[train] epoch:0000 step:0363 cost:0.069 accuracy:0.891\n",
      "[train] epoch:0000 step:0364 cost:0.048 accuracy:0.945\n",
      "[train] epoch:0000 step:0365 cost:0.048 accuracy:0.914\n",
      "[train] epoch:0000 step:0366 cost:0.067 accuracy:0.898\n",
      "[train] epoch:0000 step:0367 cost:0.043 accuracy:0.953\n",
      "[train] epoch:0000 step:0368 cost:0.059 accuracy:0.891\n",
      "[train] epoch:0000 step:0369 cost:0.061 accuracy:0.898\n",
      "[train] epoch:0000 step:0370 cost:0.053 accuracy:0.914\n",
      "[train] epoch:0000 step:0371 cost:0.047 accuracy:0.945\n",
      "[train] epoch:0000 step:0372 cost:0.051 accuracy:0.914\n",
      "[train] epoch:0000 step:0373 cost:0.056 accuracy:0.914\n",
      "[train] epoch:0000 step:0374 cost:0.044 accuracy:0.930\n",
      "[train] epoch:0000 step:0375 cost:0.060 accuracy:0.891\n",
      "[train] epoch:0000 step:0376 cost:0.047 accuracy:0.945\n",
      "[train] epoch:0000 step:0377 cost:0.050 accuracy:0.930\n",
      "[train] epoch:0000 step:0378 cost:0.053 accuracy:0.945\n",
      "[train] epoch:0000 step:0379 cost:0.050 accuracy:0.930\n",
      "[train] epoch:0000 step:0380 cost:0.057 accuracy:0.906\n",
      "[train] epoch:0000 step:0381 cost:0.041 accuracy:0.961\n",
      "[train] epoch:0000 step:0382 cost:0.045 accuracy:0.930\n",
      "[train] epoch:0000 step:0383 cost:0.082 accuracy:0.875\n",
      "[train] epoch:0000 step:0384 cost:0.055 accuracy:0.898\n",
      "[train] epoch:0000 step:0385 cost:0.055 accuracy:0.906\n",
      "[train] epoch:0000 step:0386 cost:0.055 accuracy:0.914\n",
      "[train] epoch:0000 step:0387 cost:0.081 accuracy:0.891\n",
      "[train] epoch:0000 step:0388 cost:0.070 accuracy:0.914\n",
      "[train] epoch:0000 step:0389 cost:0.073 accuracy:0.891\n",
      "[train] epoch:0000 step:0390 cost:0.041 accuracy:0.938\n",
      "[train] epoch:0000 step:0391 cost:0.060 accuracy:0.891\n",
      "[train] epoch:0000 step:0392 cost:0.060 accuracy:0.922\n",
      "[train] epoch:0000 step:0393 cost:0.049 accuracy:0.922\n",
      "[train] epoch:0000 step:0394 cost:0.058 accuracy:0.883\n",
      "[train] epoch:0000 step:0395 cost:0.048 accuracy:0.938\n",
      "[train] epoch:0000 step:0396 cost:0.076 accuracy:0.891\n",
      "[train] epoch:0000 step:0397 cost:0.060 accuracy:0.914\n",
      "[train] epoch:0000 step:0398 cost:0.043 accuracy:0.945\n",
      "[train] epoch:0000 step:0399 cost:0.047 accuracy:0.930\n",
      "[train] epoch:0000 step:0400 cost:0.054 accuracy:0.914\n",
      "[train] epoch:0000 step:0401 cost:0.057 accuracy:0.891\n",
      "[train] epoch:0000 step:0402 cost:0.048 accuracy:0.906\n",
      "[train] epoch:0000 step:0403 cost:0.041 accuracy:0.969\n",
      "[train] epoch:0000 step:0404 cost:0.049 accuracy:0.914\n",
      "[train] epoch:0000 step:0405 cost:0.058 accuracy:0.875\n",
      "[train] epoch:0000 step:0406 cost:0.050 accuracy:0.891\n",
      "[train] epoch:0000 step:0407 cost:0.053 accuracy:0.930\n",
      "[train] epoch:0000 step:0408 cost:0.037 accuracy:0.969\n",
      "[train] epoch:0000 step:0409 cost:0.038 accuracy:0.938\n",
      "[train] epoch:0000 step:0410 cost:0.066 accuracy:0.883\n",
      "[train] epoch:0000 step:0411 cost:0.041 accuracy:0.930\n",
      "[train] epoch:0000 step:0412 cost:0.039 accuracy:0.938\n",
      "[train] epoch:0000 step:0413 cost:0.077 accuracy:0.891\n",
      "[train] epoch:0000 step:0414 cost:0.066 accuracy:0.883\n",
      "[train] epoch:0000 step:0415 cost:0.051 accuracy:0.930\n",
      "[train] epoch:0000 step:0416 cost:0.052 accuracy:0.930\n",
      "[train] epoch:0000 step:0417 cost:0.063 accuracy:0.930\n",
      "[train] epoch:0000 step:0418 cost:0.079 accuracy:0.883\n",
      "[train] epoch:0000 step:0419 cost:0.047 accuracy:0.953\n",
      "[train] epoch:0000 step:0420 cost:0.060 accuracy:0.898\n",
      "[train] epoch:0000 step:0421 cost:0.045 accuracy:0.930\n",
      "[train] epoch:0000 step:0422 cost:0.053 accuracy:0.930\n",
      "[train] epoch:0000 step:0423 cost:0.053 accuracy:0.930\n",
      "[train] epoch:0000 step:0424 cost:0.061 accuracy:0.906\n",
      "[train] epoch:0000 step:0425 cost:0.044 accuracy:0.945\n",
      "[train] epoch:0000 step:0426 cost:0.061 accuracy:0.898\n",
      "[train] epoch:0000 step:0427 cost:0.044 accuracy:0.945\n",
      "[train] epoch:0000 step:0428 cost:0.065 accuracy:0.922\n",
      "[train] epoch:0000 step:0429 cost:0.049 accuracy:0.945\n",
      "[train] epoch:0000 step:0430 cost:0.049 accuracy:0.914\n",
      "[train] epoch:0000 step:0431 cost:0.057 accuracy:0.922\n",
      "[train] epoch:0000 step:0432 cost:0.047 accuracy:0.930\n",
      "[train] epoch:0000 step:0433 cost:0.056 accuracy:0.930\n",
      "[train] epoch:0000 step:0434 cost:0.067 accuracy:0.906\n",
      "[train] epoch:0000 step:0435 cost:0.055 accuracy:0.914\n",
      "[train] epoch:0000 step:0436 cost:0.041 accuracy:0.930\n",
      "[train] epoch:0000 step:0437 cost:0.075 accuracy:0.883\n",
      "[train] epoch:0000 step:0438 cost:0.063 accuracy:0.898\n",
      "[train] epoch:0000 step:0439 cost:0.060 accuracy:0.898\n",
      "[train] epoch:0000 step:0440 cost:0.052 accuracy:0.914\n",
      "[train] epoch:0000 step:0441 cost:0.047 accuracy:0.938\n",
      "[train] epoch:0000 step:0442 cost:0.039 accuracy:0.953\n",
      "[train] epoch:0000 step:0443 cost:0.037 accuracy:0.953\n",
      "[train] epoch:0000 step:0444 cost:0.054 accuracy:0.922\n",
      "[train] epoch:0000 step:0445 cost:0.069 accuracy:0.891\n",
      "[train] epoch:0000 step:0446 cost:0.047 accuracy:0.922\n",
      "[train] epoch:0000 step:0447 cost:0.045 accuracy:0.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0000 step:0448 cost:0.069 accuracy:0.867\n",
      "[train] epoch:0000 step:0449 cost:0.055 accuracy:0.906\n",
      "[train] epoch:0000 step:0450 cost:0.046 accuracy:0.930\n",
      "[train] epoch:0000 step:0451 cost:0.057 accuracy:0.938\n",
      "[train] epoch:0000 step:0452 cost:0.026 accuracy:0.977\n",
      "[train] epoch:0000 step:0453 cost:0.060 accuracy:0.914\n",
      "[train] epoch:0000 step:0454 cost:0.059 accuracy:0.906\n",
      "[train] epoch:0000 step:0455 cost:0.058 accuracy:0.906\n",
      "[train] epoch:0000 step:0456 cost:0.052 accuracy:0.891\n",
      "[train] epoch:0000 step:0457 cost:0.039 accuracy:0.953\n",
      "[train] epoch:0000 step:0458 cost:0.045 accuracy:0.938\n",
      "[train] epoch:0000 step:0459 cost:0.067 accuracy:0.906\n",
      "[train] epoch:0000 step:0460 cost:0.057 accuracy:0.891\n",
      "[train] epoch:0000 step:0461 cost:0.056 accuracy:0.898\n",
      "[train] epoch:0000 step:0462 cost:0.066 accuracy:0.875\n",
      "[train] epoch:0000 step:0463 cost:0.049 accuracy:0.922\n",
      "[train] epoch:0000 step:0464 cost:0.043 accuracy:0.961\n",
      "[train] epoch:0000 step:0465 cost:0.038 accuracy:0.953\n",
      "[train] epoch:0000 step:0466 cost:0.061 accuracy:0.875\n",
      "[train] epoch:0000 step:0467 cost:0.049 accuracy:0.945\n",
      "[train] epoch:0000 step:0468 cost:0.049 accuracy:0.938\n",
      "[0001] [train] cost:0.093 accuracy:0.838 [valid] cost:0.048 accuracy:0.930\n",
      "[train] epoch:0001 step:0001 cost:0.047 accuracy:0.930\n",
      "[train] epoch:0001 step:0002 cost:0.054 accuracy:0.938\n",
      "[train] epoch:0001 step:0003 cost:0.074 accuracy:0.875\n",
      "[train] epoch:0001 step:0004 cost:0.049 accuracy:0.938\n",
      "[train] epoch:0001 step:0005 cost:0.052 accuracy:0.922\n",
      "[train] epoch:0001 step:0006 cost:0.049 accuracy:0.906\n",
      "[train] epoch:0001 step:0007 cost:0.057 accuracy:0.898\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "history = []\n",
    "for epoch in range(30):\n",
    "    result = {}\n",
    "    \n",
    "    model.dataflow['train'].reset_state()\n",
    "    step, costs, accuracies = 0, [], []\n",
    "    for datapoint in model.dataflow['train'].get_data():\n",
    "        datapoint.append( sklearn.preprocessing.label_binarize( datapoint[1], range(10) ).astype(np.float32) )\n",
    "        _, cost, accuracy, grads = sess.run([model.op, model.cost, model.accuracy, model.grads],\n",
    "                                     feed_dict=dict(zip(model.inputs, datapoint)))\n",
    "        grads = [np.sum(np.abs(g)) for g in grads]\n",
    "        costs.append(cost)\n",
    "        accuracies.append(accuracy)\n",
    "        step += 1\n",
    "        print('[train] epoch:%04d step:%04d cost:%.3f accuracy:%0.3f'%(epoch, step, cost, accuracy))\n",
    "    print('[%04d] [train] cost:%.3f accuracy:%0.3f'%(epoch+1, np.mean(costs), np.mean(accuracies)), end=' ')\n",
    "    result['train'] = {'epoch':epoch, 'cost':np.mean(costs), 'accuracy':np.mean(accuracies), 'grads_abs':grads}\n",
    "\n",
    "    model.dataflow['valid'].reset_state()\n",
    "    costs, accuracies = [], []\n",
    "    for datapoint in model.dataflow['valid'].get_data():\n",
    "        datapoint.append( sklearn.preprocessing.label_binarize( datapoint[1], range(10) ).astype(np.float32) )\n",
    "        cost, accuracy = sess.run([model.cost, model.accuracy],\n",
    "                                     feed_dict=dict(zip(model.inputs, datapoint)))\n",
    "        costs.append(cost)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    print('[valid] cost:%.3f accuracy:%0.3f'%(np.mean(costs), np.mean(accuracies)))\n",
    "    result['valid'] = {'epoch':epoch, 'cost':np.mean(costs), 'accuracy':np.mean(accuracies)}\n",
    "    \n",
    "    history.append( result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
