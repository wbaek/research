{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorpack as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def swish(features, name=None):\n",
    "  \"\"\"Computes Self-Gated activation`.\n",
    "  Source: [SWISH: A SELF-GATED ACTIVATION FUNCTION](https://arxiv.org/pdf/1710.05941.pdf)\n",
    "  Args:\n",
    "    features: A `Tensor` with type `float`, `double`, `int32`, `int64`, `uint8`,\n",
    "      `int16`, or `int8`.\n",
    "    name: A name for the operation (optional).\n",
    "  Returns:\n",
    "    A `Tensor` with the same type as `features`.\n",
    "  \"\"\"\n",
    "  with ops.name_scope(name, \"swish\", [features]) as name:\n",
    "    features = ops.convert_to_tensor(features, name=\"features\")\n",
    "    return tf.multiply(features , tf.nn.sigmoid(features), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from tensorflow.python.ops import gradients_impl\n",
    "from tensorflow.python.ops import array_ops, tensor_array_ops, control_flow_ops\n",
    "\n",
    "def hessians_highrank(ys, xs, gradients=None, name=\"hessians\", colocate_gradients_with_ops=False,\n",
    "            gate_gradients=False, aggregation_method=None):\n",
    "  \"\"\"Constructs the Hessian (one or more rank matrix) of sum of `ys` with respect to `x` in `xs`.\n",
    "  `hessians_highrank()` adds ops to the graph to output the Hessian matrix of `ys`\n",
    "  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\n",
    "  where each tensor is the Hessian of `sum(ys)`. This function currently\n",
    "  only supports evaluating the Hessian with respect to (a list of) one-\n",
    "  dimensional tensors.\n",
    "  The Hessian is a matrix of second-order partial derivatives of a scalar\n",
    "  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n",
    "  Args:\n",
    "    ys: A `Tensor` or list of tensors to be differentiated.\n",
    "    xs: A `Tensor` or list of tensors to be used for differentiation.\n",
    "    name: Optional name to use for grouping all the gradient ops together.\n",
    "      defaults to 'hessians'.\n",
    "    colocate_gradients_with_ops: See `gradients()` documentation for details.\n",
    "    gate_gradients: See `gradients()` documentation for details.\n",
    "    aggregation_method: See `gradients()` documentation for details.\n",
    "  Returns:\n",
    "    A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.\n",
    "  Raises:\n",
    "    LookupError: if one of the operations between `xs` and `ys` does not\n",
    "      have a registered gradient function.\n",
    "  \"\"\"\n",
    "  xs = gradients_impl._AsList(xs)\n",
    "  kwargs = {\n",
    "    'colocate_gradients_with_ops': colocate_gradients_with_ops,\n",
    "    'gate_gradients': gate_gradients,\n",
    "    'aggregation_method': aggregation_method\n",
    "  }\n",
    "  # Compute first-order derivatives and iterate for each x in xs.\n",
    "  hessians = []\n",
    "  _gradients = tf.gradients(ys, xs, **kwargs) if gradients is None else gradients\n",
    "  for i, _gradient, x in zip(range(len(xs)), _gradients, xs):\n",
    "    shape = x.shape\n",
    "    _gradient = tf.reshape(_gradient, [-1])\n",
    "    \n",
    "    n = tf.size(x)\n",
    "    loop_vars = [\n",
    "      array_ops.constant(0, tf.int32),\n",
    "      tensor_array_ops.TensorArray(x.dtype, n)\n",
    "    ]\n",
    "    _, hessian = control_flow_ops.while_loop(\n",
    "      lambda j, _: j < n,\n",
    "      lambda j, result: (j + 1, result.write(j, tf.gradients(_gradient[j], x)[0])),\n",
    "      loop_vars\n",
    "    )\n",
    "    hessians.append(hessian.stack())\n",
    "  return hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorpack as tp\n",
    "\n",
    "from tensorpack import dataset\n",
    "from tensorpack.dataflow import imgaug, AugmentImageComponent, BatchData, PrefetchData\n",
    "import tensorpack.tfutils.symbolic_functions as symbf\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "class ModelMNIST10x10_simple(object):\n",
    "    def __init__(self, learning_rate=1.0, batch_size=128, momentum=0.9):\n",
    "        self.batch_size = batch_size\n",
    "        self.inputs = [\n",
    "            tf.placeholder(tf.float32, shape=(None, 10, 10, 1)),\n",
    "            tf.placeholder(tf.int32, shape=(None,)),\n",
    "            tf.placeholder(tf.float32, shape=(None, 10))\n",
    "        ]\n",
    "        \n",
    "        self.probability, self.cost, self.accuracy = self._build_graph(self.inputs)\n",
    "        self.op = self._get_optimize_operator(self.cost, learning_rate, momentum)\n",
    "        self.dataflow = {\n",
    "            'train':self._get_data('train'),\n",
    "            'valid':self._get_data('test'),\n",
    "        }\n",
    "        \n",
    "    def _build_graph(self, inputs):\n",
    "        image, label, vector = inputs\n",
    "        \n",
    "        with slim.arg_scope([slim.layers.conv2d], weights_regularizer=slim.l2_regularizer(1e-4), activation_fn=swish), \\\n",
    "             slim.arg_scope([slim.layers.fully_connected], weights_regularizer=slim.l2_regularizer(1e-5)):\n",
    "            l = image\n",
    "            #l = slim.layers.conv2d(l, 8, [3, 3], padding='SAME', scope='conv0' ) # 10x10\n",
    "            l = tf.nn.depthwise_conv2d(l, [3, 3, 3, 1], strides= padding='SAME')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool0') # 5x5\n",
    "            l = slim.layers.conv2d(l, 8, [3, 3], scope='conv1') # 3x3\n",
    "            l = slim.layers.conv2d(l, 8, [3, 3], scope='conv2') # 1x1\n",
    "            l = slim.layers.flatten(l, scope='flatten')\n",
    "            logits = slim.layers.fully_connected(l, 10, activation_fn=None, scope='fc0')\n",
    "\n",
    "        # Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\n",
    "        #cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "        cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=vector)\n",
    "        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n",
    "\n",
    "        prob = tf.nn.softmax(logits, name='prob')\n",
    "        accuracy = symbf.accuracy(logits, label, topk=1)\n",
    "        return prob, cost, accuracy\n",
    "    \n",
    "    def _get_optimize_operator(self, cost, learning_rate=1.0, momentum=0.9):\n",
    "        var_list = (tf.trainable_variables() + tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n",
    "        var_list += tf.get_collection(tf.GraphKeys._STREAMING_MODEL_PORTS)\n",
    "\n",
    "        processors = [optimizer._get_processor(v) for v in var_list]\n",
    "        var_refs = [p.target() for p in processors]\n",
    "\n",
    "        # compute_gradients\n",
    "        grads = tf.gradients(\n",
    "                cost, var_refs,\n",
    "                grad_ys=None, aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        hessis = hessians_highrank(\n",
    "                 cost, var_refs, gradients=grads,\n",
    "                 aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        \n",
    "        second_order_grads = []\n",
    "        for g, h in zip(grads, hessis):\n",
    "            shape = g.shape\n",
    "            d = int(reduce(lambda a,b: a*b, shape))\n",
    "\n",
    "            g = tf.reshape(g, [d, 1])\n",
    "            h = tf.reshape(h, [d, d]) + (tf.eye(d) * 1e-1)\n",
    "            h_inv = tf.matrix_inverse(h)\n",
    "            grad = tf.matmul(h_inv, g)\n",
    "            grad = tf.reshape(grad, shape)\n",
    "            second_order_grads.append(grad)\n",
    "        grads_and_vars = list(zip(second_order_grads, var_list))\n",
    "        \n",
    "        self.grads = grads\n",
    "        \n",
    "        self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        lr_schedule = {\n",
    "            'step':     [                   1],\n",
    "            'rate':     [0.1*learning_rate, learning_rate],\n",
    "        }\n",
    "        lr_schedule['step'] = ops.convert_n_to_tensor(lr_schedule['step'], tf.int64)\n",
    "        learning_rate = tf.train.piecewise_constant(self.global_step, lr_schedule['step'], lr_schedule['rate'])\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        if momentum >= 0.0:\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "        else:\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def _get_data(self, train_or_test):\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        isTrain = train_or_test == 'train'\n",
    "        ds = dataset.Mnist(train_or_test)\n",
    "        if isTrain:\n",
    "            augmentors = [\n",
    "                #imgaug.RandomApplyAug(imgaug.RandomResize((0.8, 1.2), (0.8, 1.2)), 0.3),\n",
    "                #imgaug.RandomApplyAug(imgaug.RotationAndCropValid(15), 0.5),\n",
    "                #imgaug.RandomApplyAug(imgaug.SaltPepperNoise(white_prob=0.01, black_prob=0.01), 0.25),\n",
    "                imgaug.Resize((10, 10)),\n",
    "                imgaug.CenterPaste((12, 12)),\n",
    "                imgaug.RandomCrop((10, 10)),\n",
    "                imgaug.MapImage(lambda x: x.reshape(10, 10, 1))\n",
    "            ]\n",
    "        else:\n",
    "            augmentors = [\n",
    "                imgaug.Resize((10, 10)),\n",
    "                imgaug.MapImage(lambda x: x.reshape(10, 10, 1))\n",
    "            ]\n",
    "        ds = AugmentImageComponent(ds, augmentors)\n",
    "        ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n",
    "        if isTrain:\n",
    "            ds = PrefetchData(ds, 3, 2)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1023 23:06:41 @fs.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n"
     ]
    }
   ],
   "source": [
    "model = ModelMNIST10x10_simple(learning_rate=0.9, momentum=0.9, batch_size=128*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session initialized\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('session initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0000 step:0001 cost:0.695 accuracy:0.052\n",
      "[train] epoch:0000 step:0002 cost:0.633 accuracy:0.112\n",
      "[train] epoch:0000 step:0003 cost:0.521 accuracy:0.109\n",
      "[train] epoch:0000 step:0004 cost:0.371 accuracy:0.096\n",
      "[0001] [train] cost:0.555 accuracy:0.092 elapsed:53.820s [valid] cost:0.356 accuracy:0.101 elapsed:0.352s\n",
      "[train] epoch:0001 step:0001 cost:0.356 accuracy:0.095\n",
      "[train] epoch:0001 step:0002 cost:0.396 accuracy:0.108\n",
      "[train] epoch:0001 step:0003 cost:0.370 accuracy:0.110\n",
      "[train] epoch:0001 step:0004 cost:0.342 accuracy:0.116\n",
      "[0002] [train] cost:0.366 accuracy:0.107 elapsed:52.173s [valid] cost:0.333 accuracy:0.113 elapsed:0.582s\n",
      "[train] epoch:0002 step:0001 cost:0.332 accuracy:0.115\n",
      "[train] epoch:0002 step:0002 cost:0.335 accuracy:0.111\n",
      "[train] epoch:0002 step:0003 cost:0.338 accuracy:0.115\n",
      "[train] epoch:0002 step:0004 cost:0.338 accuracy:0.115\n",
      "[0003] [train] cost:0.336 accuracy:0.114 elapsed:52.074s [valid] cost:0.334 accuracy:0.113 elapsed:0.306s\n",
      "[train] epoch:0003 step:0001 cost:0.334 accuracy:0.112\n",
      "[train] epoch:0003 step:0002 cost:0.330 accuracy:0.108\n",
      "[train] epoch:0003 step:0003 cost:0.326 accuracy:0.109\n",
      "[train] epoch:0003 step:0004 cost:0.325 accuracy:0.153\n",
      "[0004] [train] cost:0.329 accuracy:0.120 elapsed:52.099s [valid] cost:0.324 accuracy:0.127 elapsed:0.277s\n",
      "[train] epoch:0004 step:0001 cost:0.324 accuracy:0.124\n",
      "[train] epoch:0004 step:0002 cost:0.325 accuracy:0.160\n",
      "[train] epoch:0004 step:0003 cost:0.325 accuracy:0.142\n",
      "[train] epoch:0004 step:0004 cost:0.325 accuracy:0.150\n",
      "[0005] [train] cost:0.325 accuracy:0.144 elapsed:52.234s [valid] cost:0.324 accuracy:0.196 elapsed:0.278s\n",
      "[train] epoch:0005 step:0001 cost:0.325 accuracy:0.165\n",
      "[train] epoch:0005 step:0002 cost:0.324 accuracy:0.173\n",
      "[train] epoch:0005 step:0003 cost:0.324 accuracy:0.179\n",
      "[train] epoch:0005 step:0004 cost:0.323 accuracy:0.213\n",
      "[0006] [train] cost:0.324 accuracy:0.182 elapsed:52.059s [valid] cost:0.323 accuracy:0.230 elapsed:0.277s\n",
      "[train] epoch:0006 step:0001 cost:0.323 accuracy:0.217\n",
      "[train] epoch:0006 step:0002 cost:0.323 accuracy:0.254\n",
      "[train] epoch:0006 step:0003 cost:0.322 accuracy:0.155\n",
      "[train] epoch:0006 step:0004 cost:0.322 accuracy:0.136\n",
      "[0007] [train] cost:0.322 accuracy:0.190 elapsed:52.015s [valid] cost:0.321 accuracy:0.134 elapsed:0.276s\n",
      "[train] epoch:0007 step:0001 cost:0.322 accuracy:0.139\n",
      "[train] epoch:0007 step:0002 cost:0.321 accuracy:0.148\n",
      "[train] epoch:0007 step:0003 cost:0.321 accuracy:0.173\n",
      "[train] epoch:0007 step:0004 cost:0.320 accuracy:0.191\n",
      "[0008] [train] cost:0.321 accuracy:0.163 elapsed:52.161s [valid] cost:0.319 accuracy:0.222 elapsed:0.280s\n",
      "[train] epoch:0008 step:0001 cost:0.320 accuracy:0.211\n",
      "[train] epoch:0008 step:0002 cost:0.319 accuracy:0.246\n",
      "[train] epoch:0008 step:0003 cost:0.318 accuracy:0.275\n",
      "[train] epoch:0008 step:0004 cost:0.317 accuracy:0.304\n",
      "[0009] [train] cost:0.318 accuracy:0.259 elapsed:51.952s [valid] cost:0.315 accuracy:0.329 elapsed:0.282s\n",
      "[train] epoch:0009 step:0001 cost:0.315 accuracy:0.353\n",
      "[train] epoch:0009 step:0002 cost:0.314 accuracy:0.399\n",
      "[train] epoch:0009 step:0003 cost:0.312 accuracy:0.414\n",
      "[train] epoch:0009 step:0004 cost:0.309 accuracy:0.436\n",
      "[0010] [train] cost:0.312 accuracy:0.400 elapsed:52.134s [valid] cost:0.305 accuracy:0.443 elapsed:0.277s\n",
      "[train] epoch:0010 step:0001 cost:0.306 accuracy:0.446\n",
      "[train] epoch:0010 step:0002 cost:0.302 accuracy:0.456\n",
      "[train] epoch:0010 step:0003 cost:0.297 accuracy:0.460\n",
      "[train] epoch:0010 step:0004 cost:0.291 accuracy:0.473\n",
      "[0011] [train] cost:0.299 accuracy:0.459 elapsed:52.135s [valid] cost:0.280 accuracy:0.474 elapsed:0.277s\n",
      "[train] epoch:0011 step:0001 cost:0.283 accuracy:0.473\n",
      "[train] epoch:0011 step:0002 cost:0.275 accuracy:0.470\n",
      "[train] epoch:0011 step:0003 cost:0.265 accuracy:0.475\n",
      "[train] epoch:0011 step:0004 cost:0.254 accuracy:0.495\n",
      "[0012] [train] cost:0.269 accuracy:0.478 elapsed:52.188s [valid] cost:0.240 accuracy:0.526 elapsed:0.276s\n",
      "[train] epoch:0012 step:0001 cost:0.244 accuracy:0.510\n",
      "[train] epoch:0012 step:0002 cost:0.238 accuracy:0.515\n",
      "[train] epoch:0012 step:0003 cost:0.232 accuracy:0.534\n",
      "[train] epoch:0012 step:0004 cost:0.228 accuracy:0.534\n",
      "[0013] [train] cost:0.235 accuracy:0.523 elapsed:52.166s [valid] cost:0.216 accuracy:0.587 elapsed:0.283s\n",
      "[train] epoch:0013 step:0001 cost:0.225 accuracy:0.546\n",
      "[train] epoch:0013 step:0002 cost:0.220 accuracy:0.564\n",
      "[train] epoch:0013 step:0003 cost:0.218 accuracy:0.562\n",
      "[train] epoch:0013 step:0004 cost:0.213 accuracy:0.584\n",
      "[0014] [train] cost:0.219 accuracy:0.564 elapsed:52.446s [valid] cost:0.203 accuracy:0.622 elapsed:0.330s\n",
      "[train] epoch:0014 step:0001 cost:0.207 accuracy:0.594\n",
      "[train] epoch:0014 step:0002 cost:0.203 accuracy:0.602\n",
      "[train] epoch:0014 step:0003 cost:0.201 accuracy:0.602\n",
      "[train] epoch:0014 step:0004 cost:0.196 accuracy:0.620\n",
      "[0015] [train] cost:0.202 accuracy:0.604 elapsed:51.958s [valid] cost:0.190 accuracy:0.646 elapsed:0.327s\n",
      "[train] epoch:0015 step:0001 cost:0.191 accuracy:0.627\n",
      "[train] epoch:0015 step:0002 cost:0.188 accuracy:0.628\n",
      "[train] epoch:0015 step:0003 cost:0.189 accuracy:0.631\n",
      "[train] epoch:0015 step:0004 cost:0.185 accuracy:0.645\n",
      "[0016] [train] cost:0.188 accuracy:0.633 elapsed:52.084s [valid] cost:0.183 accuracy:0.659 elapsed:0.331s\n",
      "[train] epoch:0016 step:0001 cost:0.184 accuracy:0.641\n",
      "[train] epoch:0016 step:0002 cost:0.180 accuracy:0.658\n",
      "[train] epoch:0016 step:0003 cost:0.179 accuracy:0.663\n",
      "[train] epoch:0016 step:0004 cost:0.174 accuracy:0.675\n",
      "[0017] [train] cost:0.179 accuracy:0.659 elapsed:51.955s [valid] cost:0.176 accuracy:0.679 elapsed:0.278s\n",
      "[train] epoch:0017 step:0001 cost:0.172 accuracy:0.679\n",
      "[train] epoch:0017 step:0002 cost:0.171 accuracy:0.680\n",
      "[train] epoch:0017 step:0003 cost:0.168 accuracy:0.689\n",
      "[train] epoch:0017 step:0004 cost:0.167 accuracy:0.690\n",
      "[0018] [train] cost:0.170 accuracy:0.685 elapsed:52.206s [valid] cost:0.167 accuracy:0.704 elapsed:0.279s\n",
      "[train] epoch:0018 step:0001 cost:0.163 accuracy:0.702\n",
      "[train] epoch:0018 step:0002 cost:0.160 accuracy:0.709\n",
      "[train] epoch:0018 step:0003 cost:0.157 accuracy:0.718\n",
      "[train] epoch:0018 step:0004 cost:0.153 accuracy:0.727\n",
      "[0019] [train] cost:0.158 accuracy:0.714 elapsed:52.446s [valid] cost:0.155 accuracy:0.728 elapsed:0.328s\n",
      "[train] epoch:0019 step:0001 cost:0.152 accuracy:0.729\n",
      "[train] epoch:0019 step:0002 cost:0.151 accuracy:0.730\n",
      "[train] epoch:0019 step:0003 cost:0.147 accuracy:0.737\n",
      "[train] epoch:0019 step:0004 cost:0.144 accuracy:0.746\n",
      "[0020] [train] cost:0.149 accuracy:0.735 elapsed:52.071s [valid] cost:0.145 accuracy:0.749 elapsed:0.309s\n",
      "[train] epoch:0020 step:0001 cost:0.141 accuracy:0.755\n",
      "[train] epoch:0020 step:0002 cost:0.141 accuracy:0.757\n",
      "[train] epoch:0020 step:0003 cost:0.138 accuracy:0.764\n",
      "[train] epoch:0020 step:0004 cost:0.135 accuracy:0.772\n",
      "[0021] [train] cost:0.139 accuracy:0.762 elapsed:52.090s [valid] cost:0.135 accuracy:0.774 elapsed:0.279s\n",
      "[train] epoch:0021 step:0001 cost:0.134 accuracy:0.771\n",
      "[train] epoch:0021 step:0002 cost:0.133 accuracy:0.778\n",
      "[train] epoch:0021 step:0003 cost:0.128 accuracy:0.791\n",
      "[train] epoch:0021 step:0004 cost:0.126 accuracy:0.788\n",
      "[0022] [train] cost:0.130 accuracy:0.782 elapsed:52.314s [valid] cost:0.124 accuracy:0.800 elapsed:0.277s\n",
      "[train] epoch:0022 step:0001 cost:0.124 accuracy:0.803\n",
      "[train] epoch:0022 step:0002 cost:0.122 accuracy:0.806\n",
      "[train] epoch:0022 step:0003 cost:0.121 accuracy:0.805\n",
      "[train] epoch:0022 step:0004 cost:0.119 accuracy:0.808\n",
      "[0023] [train] cost:0.122 accuracy:0.806 elapsed:51.962s [valid] cost:0.115 accuracy:0.818 elapsed:0.279s\n",
      "[train] epoch:0023 step:0001 cost:0.116 accuracy:0.814\n",
      "[train] epoch:0023 step:0002 cost:0.114 accuracy:0.818\n",
      "[train] epoch:0023 step:0003 cost:0.112 accuracy:0.823\n",
      "[train] epoch:0023 step:0004 cost:0.109 accuracy:0.832\n",
      "[0024] [train] cost:0.113 accuracy:0.822 elapsed:52.175s [valid] cost:0.107 accuracy:0.832 elapsed:0.290s\n",
      "[train] epoch:0024 step:0001 cost:0.108 accuracy:0.834\n",
      "[train] epoch:0024 step:0002 cost:0.108 accuracy:0.832\n",
      "[train] epoch:0024 step:0003 cost:0.106 accuracy:0.838\n",
      "[train] epoch:0024 step:0004 cost:0.104 accuracy:0.840\n",
      "[0025] [train] cost:0.106 accuracy:0.836 elapsed:52.073s [valid] cost:0.100 accuracy:0.844 elapsed:0.262s\n",
      "[train] epoch:0025 step:0001 cost:0.104 accuracy:0.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch:0025 step:0002 cost:0.101 accuracy:0.847\n",
      "[train] epoch:0025 step:0003 cost:0.102 accuracy:0.844\n",
      "[train] epoch:0025 step:0004 cost:0.098 accuracy:0.850\n",
      "[0026] [train] cost:0.101 accuracy:0.845 elapsed:52.202s [valid] cost:0.096 accuracy:0.853 elapsed:0.274s\n",
      "[train] epoch:0026 step:0001 cost:0.095 accuracy:0.858\n",
      "[train] epoch:0026 step:0002 cost:0.096 accuracy:0.855\n",
      "[train] epoch:0026 step:0003 cost:0.097 accuracy:0.856\n",
      "[train] epoch:0026 step:0004 cost:0.094 accuracy:0.861\n",
      "[0027] [train] cost:0.096 accuracy:0.857 elapsed:52.173s [valid] cost:0.092 accuracy:0.859 elapsed:0.542s\n",
      "[train] epoch:0027 step:0001 cost:0.093 accuracy:0.861\n",
      "[train] epoch:0027 step:0002 cost:0.091 accuracy:0.866\n",
      "[train] epoch:0027 step:0003 cost:0.089 accuracy:0.866\n",
      "[train] epoch:0027 step:0004 cost:0.090 accuracy:0.867\n",
      "[0028] [train] cost:0.091 accuracy:0.865 elapsed:52.204s [valid] cost:0.088 accuracy:0.866 elapsed:0.284s\n",
      "[train] epoch:0028 step:0001 cost:0.089 accuracy:0.870\n",
      "[train] epoch:0028 step:0002 cost:0.087 accuracy:0.871\n",
      "[train] epoch:0028 step:0003 cost:0.085 accuracy:0.876\n",
      "[train] epoch:0028 step:0004 cost:0.084 accuracy:0.877\n",
      "[0029] [train] cost:0.086 accuracy:0.874 elapsed:52.075s [valid] cost:0.084 accuracy:0.874 elapsed:0.278s\n",
      "[train] epoch:0029 step:0001 cost:0.085 accuracy:0.878\n",
      "[train] epoch:0029 step:0002 cost:0.085 accuracy:0.876\n",
      "[train] epoch:0029 step:0003 cost:0.083 accuracy:0.877\n",
      "[train] epoch:0029 step:0004 cost:0.080 accuracy:0.884\n",
      "[0030] [train] cost:0.083 accuracy:0.879 elapsed:52.199s [valid] cost:0.081 accuracy:0.878 elapsed:0.279s\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "history = []\n",
    "for epoch in range(30):\n",
    "    result = {}\n",
    "    \n",
    "    model.dataflow['train'].reset_state()\n",
    "    step, costs, accuracies = 0, [], []\n",
    "    \n",
    "    timestamp = time.time()\n",
    "    for datapoint in model.dataflow['train'].get_data():\n",
    "        datapoint.append( sklearn.preprocessing.label_binarize( datapoint[1], range(10) ).astype(np.float32) )\n",
    "        _, cost, accuracy, grads = sess.run([model.op, model.cost, model.accuracy, model.grads],\n",
    "                                     feed_dict=dict(zip(model.inputs, datapoint)))\n",
    "        grads = [np.sum(np.abs(g)) for g in grads]\n",
    "        costs.append(cost)\n",
    "        accuracies.append(accuracy)\n",
    "        step += 1\n",
    "        print('[train] epoch:%04d step:%04d cost:%.3f accuracy:%0.3f'%(epoch, step, cost, accuracy))\n",
    "    eplapsed = time.time() - timestamp\n",
    "    print('[%04d] [train] cost:%.3f accuracy:%0.3f elapsed:%.3fs'%(epoch+1, np.mean(costs), np.mean(accuracies), eplapsed), end=' ')\n",
    "    result['train'] = {'epoch':epoch, 'cost':np.mean(costs), 'accuracy':np.mean(accuracies), 'grads_abs':grads}\n",
    "\n",
    "    model.dataflow['valid'].reset_state()\n",
    "    costs, accuracies = [], []\n",
    "    timestamp = time.time()\n",
    "    for datapoint in model.dataflow['valid'].get_data():\n",
    "        datapoint.append( sklearn.preprocessing.label_binarize( datapoint[1], range(10) ).astype(np.float32) )\n",
    "        cost, accuracy = sess.run([model.cost, model.accuracy],\n",
    "                                     feed_dict=dict(zip(model.inputs, datapoint)))\n",
    "        costs.append(cost)\n",
    "        accuracies.append(accuracy)\n",
    "    eplapsed = time.time() - timestamp\n",
    "    print('[valid] cost:%.3f accuracy:%0.3f elapsed:%.3fs'%(np.mean(costs), np.mean(accuracies), eplapsed))\n",
    "    result['valid'] = {'epoch':epoch, 'cost':np.mean(costs), 'accuracy':np.mean(accuracies)}\n",
    "    \n",
    "    history.append( result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4*3))\n",
    "for i, label in enumerate(['cost', 'accuracy', 'grads_abs']):\n",
    "    ax = fig.add_subplot(3, 1, i+1)\n",
    "    ax.set_xlabel('epoch', fontsize=12)\n",
    "    ax.set_ylabel(label, fontsize=12)\n",
    "    ax.set_title(label, fontsize=12)\n",
    "    for j, mode in enumerate(['train', 'valid']):\n",
    "        if not label in history[0][mode]:\n",
    "            continue\n",
    "        ax.plot(np.array([h[mode]['epoch'] for h in history]), np.array([np.sum(h[mode][label]) for h in history]), label=mode)\n",
    "    ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
