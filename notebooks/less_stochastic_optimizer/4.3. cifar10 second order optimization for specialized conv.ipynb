{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorpack as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from tensorflow.python.ops import gradients_impl\n",
    "from tensorflow.python.ops import array_ops, tensor_array_ops, control_flow_ops\n",
    "\n",
    "def hessians_for_conv(ys, xs, gradients=None, name=\"hessians\", colocate_gradients_with_ops=False,\n",
    "            gate_gradients=False, aggregation_method=None):\n",
    "  \"\"\"Constructs the Hessian (one or more rank matrix) of sum of `ys` with respect to `x` in `xs`.\n",
    "  `hessians_highrank()` adds ops to the graph to output the Hessian matrix of `ys`\n",
    "  with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\n",
    "  where each tensor is the Hessian of `sum(ys)`. This function currently\n",
    "  only supports evaluating the Hessian with respect to (a list of) one-\n",
    "  dimensional tensors.\n",
    "  The Hessian is a matrix of second-order partial derivatives of a scalar\n",
    "  tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n",
    "  Args:\n",
    "    ys: A `Tensor` or list of tensors to be differentiated.\n",
    "    xs: A `Tensor` or list of tensors to be used for differentiation.\n",
    "    name: Optional name to use for grouping all the gradient ops together.\n",
    "      defaults to 'hessians'.\n",
    "    colocate_gradients_with_ops: See `gradients()` documentation for details.\n",
    "    gate_gradients: See `gradients()` documentation for details.\n",
    "    aggregation_method: See `gradients()` documentation for details.\n",
    "  Returns:\n",
    "    A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.\n",
    "  Raises:\n",
    "    LookupError: if one of the operations between `xs` and `ys` does not\n",
    "      have a registered gradient function.\n",
    "  \"\"\"\n",
    "  xs = gradients_impl._AsList(xs)\n",
    "  kwargs = {\n",
    "    'colocate_gradients_with_ops': colocate_gradients_with_ops,\n",
    "    'gate_gradients': gate_gradients,\n",
    "    'aggregation_method': aggregation_method\n",
    "  }\n",
    "\n",
    "  def _parallel_gradients(grad, x, **kwargs):\n",
    "    n = tf.size(x)\n",
    "    \n",
    "    loop_vars = [\n",
    "      array_ops.constant(0, tf.int32),\n",
    "      tensor_array_ops.TensorArray(x.dtype, n)\n",
    "    ]\n",
    "    _, hessian = control_flow_ops.while_loop(\n",
    "      lambda j, _: j < n,\n",
    "      lambda j, result: (j + 1, result.write(j, tf.gradients(_gradient[j], x)[0])),\n",
    "      loop_vars\n",
    "    )\n",
    "    return hessian\n",
    "\n",
    "  # Compute first-order derivatives and iterate for each x in xs.\n",
    "  hessians = []\n",
    "  _gradients = tf.gradients(ys, xs, **kwargs) if gradients is None else gradients\n",
    "  for i, _gradient, x in zip(range(len(xs)), _gradients, xs):\n",
    "    shape = x.shape\n",
    "    if len(shape) == 4: # for conv\n",
    "      _gradient = tf.reshape(_gradient, [-1, int(shape[-1])])\n",
    "      hs = []\n",
    "      for gradient_index in range(shape[-1]):\n",
    "        g = _gradient[:, gradient_index]\n",
    "        h = _parallel_gradients(g, x)\n",
    "        h = h.stack()[:,:,:,gradient_index]\n",
    "        hs.append(h)\n",
    "        print(h.shape)\n",
    "      hessian = tf.stack(hs, -1).reshape([-1] + shape)\n",
    "    else:\n",
    "      _gradient = tf.reshape(_gradient, [-1])\n",
    "      hessian = _parallel_gradients(_gradient, x)\n",
    "    hessians.append(hessian.stack())\n",
    "  return hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorpack import dataset\n",
    "from tensorpack.dataflow import imgaug, AugmentImageComponent, BatchData, PrefetchData\n",
    "import tensorpack.tfutils.symbolic_functions as symbf\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "class ModelCIFAR10_simple(object):\n",
    "    def __init__(self, learning_rate=1.0, batch_size=16):\n",
    "        self.batch_size = batch_size\n",
    "        self.inputs = [\n",
    "            tf.placeholder(tf.float32, shape=(None, 32, 32, 3)),\n",
    "            tf.placeholder(tf.int32, shape=(None, )),\n",
    "            tf.placeholder(tf.float32, shape=(None, 10))\n",
    "        ]\n",
    "        \n",
    "        self.probability, self.cost, self.accuracy = self._build_graph(self.inputs)\n",
    "        self.op = self._get_optimize_operator(self.cost, learning_rate)\n",
    "        self.dataflow = {\n",
    "            'train':self._get_data('train', self.batch_size),\n",
    "            'valid':self._get_data('test', self.batch_size),\n",
    "        }\n",
    "        \n",
    "    def _build_graph(self, inputs):\n",
    "        image, label, vector = inputs\n",
    "        \n",
    "        with slim.arg_scope([slim.layers.fully_connected], weights_regularizer=slim.l2_regularizer(1e-5)):\n",
    "            l = slim.layers.conv2d(image, 32, [3, 3], scope='conv0')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool0')\n",
    "            l = slim.layers.conv2d(l, 32, [3, 3], padding='SAME', scope='conv1')\n",
    "            l = slim.layers.conv2d(l, 32, [3, 3], scope='conv2')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool1')\n",
    "            l = slim.layers.conv2d(l, 32, [3, 3], scope='conv3')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool2')\n",
    "            l = slim.layers.conv2d(l, 32, [3, 3], scope='conv4')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool3')\n",
    "            l = slim.layers.conv2d(l, 32, [3, 3], scope='conv5')\n",
    "            l = slim.layers.max_pool2d(l, [2, 2], scope='pool4')\n",
    "            l = slim.layers.flatten(l, scope='flatten')\n",
    "            l = slim.layers.fully_connected(l, 512, scope='fc0')\n",
    "            logits = slim.layers.fully_connected(l, 10, activation_fn=None, scope='fc1')\n",
    "\n",
    "        # Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\n",
    "        #cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "        cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=vector)\n",
    "        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n",
    "\n",
    "        prob = tf.nn.softmax(logits, name='prob')\n",
    "        accuracy = symbf.accuracy(logits, label, topk=1)\n",
    "        return prob, cost, accuracy\n",
    "    \n",
    "    def _get_optimize_operator(self, cost, learning_rate=0.1):\n",
    "        var_list = (tf.trainable_variables() + tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n",
    "        var_list += tf.get_collection(tf.GraphKeys._STREAMING_MODEL_PORTS)\n",
    "\n",
    "        processors = [optimizer._get_processor(v) for v in var_list]\n",
    "        var_refs = [p.target() for p in processors]\n",
    "        \n",
    "        # compute_gradients\n",
    "        grads = tf.gradients(\n",
    "                cost, var_refs,\n",
    "                grad_ys=None, aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        hessis = hessians_for_conv(\n",
    "                 cost, var_refs, gradients=grads,\n",
    "                 aggregation_method=None, colocate_gradients_with_ops=True)\n",
    "        \n",
    "        second_order_grads = []\n",
    "        for g, h in zip(grads, hessis):\n",
    "            shape = g.shape\n",
    "            d = int(reduce(lambda a,b: a*b, shape))\n",
    "\n",
    "            g = tf.reshape(g, [d, 1])\n",
    "            h = tf.reshape(h, [d, d]) + (tf.eye(d) * 1e-1)\n",
    "            h_inv = tf.matrix_inverse(h)\n",
    "            grad = tf.matmul(h_inv, g)\n",
    "            grad = tf.reshape(grad, shape)\n",
    "            second_order_grads.append(grad)\n",
    "        grads_and_vars = list(zip(second_order_grads, var_list))\n",
    "        \n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    def _get_data(self, train_or_test, batch_size):\n",
    "        isTrain = train_or_test == 'train'\n",
    "        ds = dataset.Cifar10(train_or_test)\n",
    "        pp_mean = ds.get_per_pixel_mean()\n",
    "        if isTrain:\n",
    "            augmentors = [\n",
    "                imgaug.CenterPaste((40, 40)),\n",
    "                imgaug.RandomCrop((32, 32)),\n",
    "                imgaug.Flip(horiz=True),\n",
    "                imgaug.MapImage(lambda x: x - pp_mean),\n",
    "            ]\n",
    "        else:\n",
    "            augmentors = [\n",
    "                imgaug.MapImage(lambda x: x - pp_mean)\n",
    "            ]\n",
    "        ds = AugmentImageComponent(ds, augmentors)\n",
    "        ds = BatchData(ds, batch_size, remainder=not isTrain)\n",
    "        if isTrain:\n",
    "            ds = PrefetchData(ds, 3, 2)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelCIFAR10_simple(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
